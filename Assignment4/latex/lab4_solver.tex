\documentclass[12pt]{article}
\renewcommand{\baselinestretch}{1.5}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\hypersetup{linktoc=all}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{graphicx}
\usepackage{float}

\begin{document}

\title{Parallel Processing Lab 4: \\Equation Solver}
\author{Sarah Peachey \& Nathan Schomer}
\maketitle


\textbf{\textit{Abstract:}} Solving a simple partial
differential equation on a grid involves a simple set 
of operations which need to be performed on each element
in the $n \times n$ grid. Each element is replaced
with a weighted 
average of itself and it's 4 adjacent, non-diagonal 
neighbors. This process is performed iteratively 
until convergence. This is when
the sum of the operation's effects on the grid 
drop below a certain threshold.
When this operation is performed in-place
on the grid, it's known as the Gauss-Seidel method.
In order to parallelize this process, the operations
can be performed in parallel using the Jacobi method.
This leverages a "ping pong" style design by calculating
each element independently of the others and then performing
another iteration on the result. The Jacobi method also
continues until the total difference of any iteration 
drops below a set threshold.



\newpage
\vspace{-1.5cm}
\section{Naive Design}
\vspace{-0.25cm}

Both the naive and the shared kernel designs implemented the Jacobi 
method since it's operations can be easily parallelized. The naive
design used only global memory. A $4 \times 4$ grid of $32 \times 32$
thread blocks was created yielding a total of $16,384$ threads. 
The smallest matrix to be tested is $1024 \times 1024$ with a total
of $1048576$ elements. Since the threads available is less than 
the matrix size, striding was required. For each iteration of the 
naive Jacobi solver, each thread calculated the weighted sum of itself
with its 4 adjacent elements from the source matrix. 
The result is stored in the destination matrix.

%\begin{equation}
%    dest[y][x] = 0.2*(current + top + left + right + below)
%\end{equation}

The difference between the previous and the new destination value 
is then added to a running sum of difference. Once all elements for
the current matrix are calculated, the total difference is read back
to the CPU and compared to a threshold. If the difference is still higher
than the threshold, the source and destination matrices are swapped
and the kernel is called again.

Note - These operations are only performed for the inner (non-border) elements
of the matrix.

\vspace{1cm}
\begin{algorithm}[H]
	\KwData{src, dest, diff}
 	\KwResult{kernel to calculate Jacobi of src and store in dest}
    find current location in matrix\;
    calculate stride length\;

    \For{ty = number of strides}{
        \For{tx = number of strides}{
            tmp = dest[tidy + ty*strideLen][tidx + tx*strideLen]\;
            calculate weighted sum\;
            diff = weighted sum - tmp\;
        }
    }
\end{algorithm}

\newpage
\vspace{-1.5cm}
\section{Shared Design}
\vspace{-0.25cm}

While the naive Jacobi kernel improved upon the serial CPU design,
additional improvements can still be had. The most obvious deficiency
of the naive kernel is the multiple accesses of each element within
the source matrix. In the naive kernel, each non-border 
element is accessed 5 times. These constant accesses of global memory
substantially decreases the arithmetic intensity. Because individual 
matrix elements are being access multiple times, we can reduce 
the number of global memory reads by utilizing shared memory. 

In the shared memory design, each thread in the current thread block
reads it's corresponding element from global memory into shared memory. 
Next, each thread will calculate the weighted average just as the naive 
kernel but using shared memory instead of global. The difference 
is still stored in global memory since it's only being written to.

The only additional difference is the stride length. In the shared memory
design, stride length is decreased by 1 because the tile needs 


% TODO: write shared pseudo code
\vspace{1cm}
\begin{algorithm}[H]
	\KwData{src, dest, diff}
 	\KwResult{kernel to calculate Jacobi using shared memory}
    allocate shared memory for src\;
    find current location in matrix\;
    calculate stride length\;
    
    copy element from global to shared\;

    wait for all threads to finish copying\;

    \For{ty = number of strides}{
        \For{tx = number of strides}{
            tmp = dest[tidy + ty*strideLen][tidx + tx*strideLen]\;
            calculate weighted sum using shared memory\;
            diff = weighted sum - tmp\;
        }
    }
\end{algorithm}


\pagebreak
\vspace{-0.6cm}
\section{Results}
\vspace{-0.4cm}

% TODO: describe results
As expected, large speedups were obtained with both kernel versions.
The shared memory kernel yielded a small additional speedup over the
naive kernel. Also the larger the matrix, the larger the speedups
over the CPU version. Speedup was calculated using Equation 1.

As seen in Table 1 and Table 2, the majority of the kernel execution time
was taken up by the initial data transfer to the GPU. Without including
the data transfer in the timing, speed-ups were much higher.

The kernels were originally created with thread blocks of size $32 \times 32$
with a grid size of $4 \times 4$. Adjusting that to 
thread blocks of sizes $16 \times 16$ and a grid of size $8 \times 8$. 
The smaller thread block with a larger grid yielded a speedup of 
0.07 when the matrix was of size $4096 \times 4096$ using the
shared memory kernel. The speedup for the same size matrix but with
the naive kernel was 0.99. The speedup values less than one indicate
that the smaller thread block size slows the kernels down. However,
the change is so minute that the block size didn't really have much effect
on run time. Large gains over the CPU version were still had.

\begin{equation}
    s = \frac{t_{serial}}{t_{parallel}}\label{eq1}
\end{equation}


% TODO: update results
\begin{table}[H]
\centering
\begin{tabular}{@{}|l|c|c|}
\hline
Matrix Size (squared) & Naive Speed-Up  & Shared Memory Speed-Up \\ \hline
$2048 \times 2048$      & 938.96          & 940.81  \\ \hline 
$4096 \times 4096$      & 3201.55         & 3692.43 \\ \hline 
$8192 \times 8192$      & 8533.79         & 9218.82 \\ \hline 
\end{tabular}
\caption{Speed-Up calculated on Xunil-05. Doesn't include initial data transfer
time}
\end{table}

% TODO: update results
\begin{table}[H]
\centering
\begin{tabular}{@{}|l|c|c|}
\hline
Matrix Size (squared) & Naive Speed-Up  & Shared Memory Speed-Up \\ \hline
$2048 \times 2048$      & 135.33          & 297.87  \\ \hline 
$4096 \times 4096$      & 236.70          & 370.58 \\ \hline 
$8192 \times 8192$      & 326.95          & 324.87 \\ \hline 
\end{tabular}
\caption{Speed-Up calculated on Xunil-05 includes data transfer time}
\end{table}


\end{document}
