\documentclass[12pt]{article}
\renewcommand{\baselinestretch}{1.5}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\hypersetup{linktoc=all}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{graphicx}
\usepackage{float}

\begin{document}

\title{Parallel Processing Lab 4: \\Equation Solver}
\author{Sarah Peachey \& Nathan Schomer}
\maketitle


\textbf{\textit{Abstract:}} Solving a simple partial
differential equation on a grid involves a simple set 
of operations which need to be performed on each element
in the $n \times n$ grid. Each element is replaced
with a weighted 
average of itself and it's 4 adjacent, non-diagonal 
neighbors. This process is performed iteratively 
until convergence. This is when
the sum of the operation's effects on the grid 
drop below a certain threshold.
When this operation is performed in-place
on the grid, it's known as the Gauss-Seidel method.
In order to parallelize this process, the operations
can be performed in parallel using the Jacobi method.
This leverages a "ping pong" style design by calculating
each element independently of the others and then performing
another iteration on the result. The Jacobi method also
continues until the total difference of any iteration 
drops below a set threshold.



\newpage
\vspace{-1.5cm}
\section{Naive Design}
\vspace{-0.25cm}

Both the naive and the shared kernel designs implemented the Jacobi 
method since it's operations can be easily parallelized. The naive
design used only global memory. A $4 \times 4$ grid of $32 \times 32$
thread blocks was created yielding a total of $16,384$ threads. 
The smallest matrix to be tested is $1024 \times 1024$ with a total
of $1048576$ elements. Since the threads available is less than 
the matrix size, striding was required. For each iteration of the 
naive Jacobi solver, each thread calculated the weighted sum of itself
with its 4 adjacent elements from the source matrix. 
The result is stored in the destination matrix.

%\begin{equation}
%    dest[y][x] = 0.2*(current + top + left + right + below)
%\end{equation}

The difference between the previous and the new destination value 
is then added to a running sum of difference. Once all elements for
the current matrix are calculated, the total difference is read back
to the CPU and compared to a threshold. If the difference is still higher
than the threshold, the source and destination matrices are swapped
and the kernel is called again.

Note - These operations are only performed for the inner (non-border) elements
of the matrix.

\vspace{1cm}
\begin{algorithm}[H]
	\KwData{src, dest, diff}
 	\KwResult{kernel to calculate Jacobi of src and store in dest}
    find current location in matrix\;
    calculate stride length\;

    \For{ty = number of strides}{
        \For{tx = number of strides}{
            tmp = dest[tidy + ty*strideLen][tidx + tx*strideLen]\;
            calculate weighted sum\;
            diff = weighted sum - tmp\;
        }
    }
\end{algorithm}

\newpage
\vspace{-1.5cm}
\section{Shared Design}
\vspace{-0.25cm}

While the naive Jacobi kernel improved upon the serial CPU design,
additional improvements can still be had. The most obvious deficiency
of the naive kernel is the multiple accesses of each element within
the source matrix. In the naive kernel, each non-border 
element is accessed 5 times. These constant accesses of global memory
substantially decreases the arithmetic intensity. Because individual 
matrix elements are being access multiple times, we can reduce 
the number of global memory reads by utilizing shared memory. 

In the shared memory design, each thread in the current thread block
reads it's corresponding element from global memory into shared memory. 
Next, each thread will calculate the weighted average just as the naive 
kernel but using shared memory instead of global. The difference 
is still stored in global memory since it's only being written to.

The only additional difference is the stride length. In the shared memory
design, stride length is decreased by 1 because the tile needs 


% TODO: write shared pseudo code
\vspace{1cm}
\begin{algorithm}[H]
	\KwData{src, dest, diff}
 	\KwResult{kernel to calculate Jacobi using shared memory}
    allocate shared memory for src\;
    find current location in matrix\;
    calculate stride length\;
    
    copy element from global to shared\;

    wait for all threads to finish copying\;

    \For{ty = number of strides}{
        \For{tx = number of strides}{
            tmp = dest[tidy + ty*strideLen][tidx + tx*strideLen]\;
            calculate weighted sum using shared memory\;
            diff = weighted sum - tmp\;
        }
    }
\end{algorithm}


\pagebreak
\vspace{-0.6cm}
\section{Results}
\vspace{-0.4cm}

% TODO: describe results

\begin{equation}
    s = \frac{t_{serial}}{t_{parallel}}\label{eq1}
\end{equation}


% TODO: update results
\begin{table}[H]
\centering
\begin{tabular}{@{}|l|c|c|}
\hline
Number of threads and Elements& Speed-Up with constant & Speed-Up without \\ \hline
4096 $10^5$  & 1.278  & 1.545 \\ \hline 
4096 $10^6$  & 2.370  & 2.708 \\ \hline 
4096 $10^7$  & 2.473  & 2.503 \\ \hline 
1024 $10^5$  & 0.698  & 0.758 \\ \hline
1024 $10^6$  & 0.747  & 0.780 \\ \hline
1024 $10^7$  & 0.634  & 0.639 \\ \hline
\end{tabular}
\caption{Speed-Up calculated on Xunil-05}
\end{table}

\end{document}
