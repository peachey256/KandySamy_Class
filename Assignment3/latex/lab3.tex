\documentclass[12pt]{article}
\renewcommand{\baselinestretch}{1.5}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\hypersetup{linktoc=all}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{graphicx}
\usepackage{float}

\begin{document}

\title{Parallel Processing Lab 3: \\ Introduction to CUDA - Matrix-Vector
Multiplication}
\author{Sarah Peachey \& Nathan Schomer}
\maketitle


\textbf{\textit{Abstract:}} GPU's can process SIMD code very quickly because
it can run many threads on different data with the same instructions. First
the CPU must create resources on the GPU and copy the data over, then the
kernel code is launched. Once the kernel is launched the CPU can either run
more code in parallel with the GPU or it can wait for the GPU kernel to
complete. Then the CPU would transfer the output from the GPU back to the CPU 
and then free the resources on the GPU. The kernel code is ran on every
thread in the thread blocks in the grid, which was allocated by CUDA
functions in the CPU code.   
\newpage

\vspace{-1.5cm}
\section{Naive Design}
\vspace{-0.25cm}


\qquad To calculate the matrix vector multiplication of $Ax=y$ where A is a
$n \times n$ matrix, x is a $n \times 1$ vector, and y is a $n \times 1$
vector. The naive approach is to create a thread to compute each element in
the y vector. That thread then loads all the elements of the x vector as
well as all the elements in that corresponding row of A. Then element-wise
multiply and sum those values and store it back into global memory in the
vector y. 

So two values are loaded from global memory for every two floating
point operations, so the arithmetic intensity of the algorithm is 1. Which
means the code is memory bound, because it only uses 10 of the available 8800
GFLOPs. To achieve the peak
performance rate 880 floating point operations must be computed per load
operation, since the GTX 1080 GPU has a peak processing rate of 8800 GFLOPs and
the memory bandwidth on the device is 320 GB/s. Another algorithm will be
applied in the next section that uses shared memory to increase the
arithmetic intensity. 

\vspace{1cm}
\begin{algorithm}[H]
	\KwData{Ad, Xd, Yd}
 	\KwResult{kernel to calculate the matrix vector multiplication}
 	initialization\;
	tid=threadIdx.y+(blockIdx.y*blockDim.y)\;
	yTemp=0\;
    \For{\texttt{i < MATRIX SIZE }}{
  	    yTemp+=A[tid*MATRIX SIZE+i]*X[i]\;
   	}
	y[tid]=yTemp\; 
   	
% \caption{Pthreads Numerical Integration}
\end{algorithm}

\pagebreak
\vspace{-0.6cm}
\section{Shared Memory Design}

\vspace{-0.4cm}

%% PUT WORDS HERE %%
The shared memory version of the matrix-vector multiplication
algorithm described above takes the same inputs and produces
the same result. However, this result is calculated in small
pieces known as "tiles". This tiled approach to matrix-vector
multiplication leverages spatial locality of the operands.
For a tile of size $n \times n$, a thread block of size $n \times n$
will be created and each thread will load an element from the
vector into shared memory 
and a single column of these threads will load elements from the
vector into shared memory. The number of thread blocks (block grid
height) will be calculated with $\frac{MATRIX\_HEIGHT}{TILE\_SIZE}$.
An additional tile will be added to the grid if these are not evenly
divisible. 

Once in shared memory, the threads can perform matrix-vector
multiplication on the current tile and add the partial sum
to the resultant vector. Once all tiles are calculated,
the result vector (of size $n \times 1$) will contain the result.
A snippet of the kernel is included on the following page.

The arithmetic intensity of this is much higher since the number
of loads from global memory is reduced but the number of floating
point operations remain the same. This can be calculated by 
$\frac{2\times n\times n}{n\times n+n}$. When $n=1024$ the arithmetic
intensity is approximately $2$\dots\ a $2\times$ improvement over the naive
method. This results in a performance of 20 FLOPS.

%%%%%%%%%%%%%%%%%%%%

\newpage
\vspace{1cm}
\begin{verbatim}
// moves tile across matrix
for(k=0; k<MATRIX_SIZE; k+=TILE_SIZE) {
    // check M edge conditions for this tile
    if(k + tileCol < MATRIX_SIZE && row < MATRIX_SIZE)
        M_shared[tileRow][tileCol] = Ad[row*MATRIX_SIZE + k + tileCol];
    else
        M_shared[tileRow][tileCol] = 0.0f;

    if (k + tileCol < MATRIX_SIZE)
        N_shared[tileCol] = Xd[k+tileCol];
    else
        N_shared[tileCol] = 0.0f;

    __syncthreads();

    for(temp = 0; temp < TILE_SIZE; temp++)
        partSum += M_shared[tileRow][temp] * N_shared[temp];

    __syncthreads();
}

if (row < MATRIX_SIZE)
    Yd[row] = (float)partSum;
\end{verbatim}

\pagebreak
\vspace{-0.6cm}
\section{Discussion of speed up}
\vspace{-0.4cm}

\qquad Speed-up was calculated for each version of the kernel by dividing the 
serial run-time by the parallelized run-time as seen in Equation 1. 
As seen by the speed-up ratios in Table 1, a trend of increased speed-up
with increased matrix size was found. This is to be expected since
an increased number of FLOPs on the GPU decreases the impact of kernel initialization
and related GPGPU overhead.

\begin{equation}
    s = \frac{t_{serial}}{t_{parallel}}\label{eq1}
\end{equation}

\begin{table}[H]
\centering
\begin{tabular}{@{}|l|c|c|}
\hline
Size & Global Speed-Up & Shared Speed-Up\\ \hline
$512\times 512$    & 3.85 & 6.42 \\ \hline 
$1024\times 1024$  & 3.02 & 4.78 \\ \hline 
$2048\times 2048$  & 6.73 & 5.20 \\ \hline 
\end{tabular}
\caption{Speed-Up calculated on Xunil-05}
\end{table}


\end{document}
